{
  "hash": "8a97ff4bc29250fa488cc07790d82968",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"measr 0.2.1\"\ndate: 2023-04-10\ndescription: \"Initial release of measr package for estimating and evaluating diagnostic classification models.\"\nimage: \"featured.jpg\"\nimage-alt: \"Measurement tools on a desk\"\ntitle-block-banner: featured.jpg\ntwitter-card:\n  image: \"featured.jpg\"\nopen-graph:\n  image: \"featured.jpg\"\nengine: knitr\ncitation: true\n# one of: \"deep-dive\", \"learn\", \"package\", or \"other\" + relevant packages\ncategories:\n  - package\n  - measr\n---\n\n\n\n\n\nWe're excited to announce the release of [measr](https://measr.info) 0.2.1.\nThe goal of measr is to provide applied researchers and psychometricians with a user friendly interface for estimating and evaluating diagnostic classification models (DCMs).\nDCMs are a class of psychometric models that provide classification of students into profiles of proficiency on a predefined set of knowledge, skills, or understandings.\nThis offers many advantages over traditional assessment models.\nBecause results are reported as proficiency on individual skills, teachers, students, and parents get actionable feedback about which areas could use additional attention.\nHowever, these models have not been widely used in applied or operational settings, in part due to a lack of user friendly software for estimating and evaluating these models.\nmeasr aims to bridge this gap between psychometric theory and applied practice.\n\nYou can install it from CRAN with:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstall.packages(\"measr\")\n```\n:::\n\n\n\nThis blog post will highlight the main features of the package.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(measr)\n```\n:::\n\n\n\n## Estimating DCMs\n\nTo illustrate DCMs and their application with measr, we'll use a simulated data set.\nIn this data set, we have 1,000 respondents who each answered 15 questions about addition, multiplication, and fractions.\nFor each item, a 1 represents a correct response to the item, and a 0 represents an incorrect response.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndat <- read_csv(\"data/example-data.csv\")\ndat\n#> # A tibble: 1,000 × 16\n#>    resp_id item_01 item_02 item_03 item_04 item_05 item_06 item_07 item_08\n#>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#>  1       1       1       1       1       0       1       1       0       1\n#>  2       2       1       1       0       0       0       1       0       1\n#>  3       3       1       1       0       0       0       1       0       1\n#>  4       4       1       0       1       0       1       0       1       1\n#>  5       5       1       1       0       0       0       1       1       0\n#>  6       6       1       1       0       0       0       1       0       1\n#>  7       7       1       1       1       0       0       1       1       1\n#>  8       8       0       0       0       0       0       0       0       1\n#>  9       9       0       1       0       0       0       1       0       0\n#> 10      10       1       1       0       1       1       1       0       1\n#> # ℹ 990 more rows\n#> # ℹ 7 more variables: item_09 <dbl>, item_10 <dbl>, item_11 <dbl>,\n#> #   item_12 <dbl>, item_13 <dbl>, item_14 <dbl>, item_15 <dbl>\n```\n:::\n\n\n\nWhen using DCMs, a Q-matrix defines which items measure each attribute, or skill.\nIn the Q-matrix, a 1 indicates that the item measures the attributes, and a 0 indicates the item does not measure the attribute.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqmatrix <- read_csv(\"data/example-qmatrix.csv\")\nqmatrix\n#> # A tibble: 15 × 4\n#>    item_id addition multiplication fractions\n#>    <chr>      <dbl>          <dbl>     <dbl>\n#>  1 item_01        1              1         0\n#>  2 item_02        1              0         0\n#>  3 item_03        0              1         0\n#>  4 item_04        0              0         1\n#>  5 item_05        0              1         0\n#>  6 item_06        1              0         0\n#>  7 item_07        0              1         0\n#>  8 item_08        0              1         1\n#>  9 item_09        0              1         1\n#> 10 item_10        0              0         1\n#> 11 item_11        1              1         0\n#> 12 item_12        1              0         1\n#> 13 item_13        0              0         1\n#> 14 item_14        0              1         1\n#> 15 item_15        1              0         0\n```\n:::\n\n\n\nOur goal is to estimate whether each respondent is proficient in each of the skills measured by the assessment, and DCMs are our tool.\nThere are many different types of DCMs, each with different constraints or assumptions about how the attributes relate to each other.\nFor example, if an item measures multiple attributes, does a respondent need to be proficient on all attributes to answer the item correctly?\nOr can proficiency in one attribute compensate for the lack of proficiency on another?\n\nAlso several DCM subtypes are supported by measr, we are primarily focused on the [loglinear cognitive diagnostic model](https://link.springer.com/article/10.1007/s11336-008-9089-5) (LCDM), which is a general DCM.\nThis means that it subsumes many of the other DCM subtypes and allows for the data to determine how much (if at all) one attribute might compensate for another.\n\nmeasr works by wrapping the [*Stan*](https://mc-stan.org) language to estimate DCMs.\nWe can estimate the LCDM using `measr_dcm()`.\nThis function use the inputs to write a Stan script defining the model and then estimates the model using either the [rstan](https://mc-stan.org/rstan/) or [cmdstanr](https://mc-stan.org/cmdstanr/) packages.\nTo estimate the model, we just supply our data set and the Q-matrix.\nBecause our data set and Q-matrix contain respondent and item identifiers, respectively, we need to specify the names of the identifier columns.\nWe can also specify prior distributions for each of the model parameters.\nFinally, we specify a file for saving the model once it is estimated.\nFor more information on model estimation, including details on specifying prior distributions, see the [model estimation vignette](https://measr.info/articles/model-estimation.html).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlcdm <- measr_dcm(data = dat, qmatrix = qmatrix,\n                  resp_id = \"resp_id\", item_id = \"item_id\", \n                  prior = c(prior(normal(0, 2), class = \"intercept\"),\n                            prior(lognormal(0, 1), class = \"maineffect\"),\n                            prior(normal(0, 2), class = \"interaction\")),\n                  file = \"fits/example-lcdm\")\n```\n:::\n\n\n\nOnce the model has estimated, we can use `measr_extract()` to examine different aspects of the model, such as proportion of respondents with each pattern of proficiency across the attributes.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasr_extract(lcdm, \"strc_param\")\n#> # A tibble: 8 × 2\n#>   class          estimate\n#>   <chr>        <rvar[1d]>\n#> 1 [0,0,0]  0.165 ± 0.0127\n#> 2 [1,0,0]  0.211 ± 0.0153\n#> 3 [0,1,0]  0.048 ± 0.0089\n#> 4 [0,0,1]  0.072 ± 0.0090\n#> 5 [1,1,0]  0.166 ± 0.0164\n#> 6 [1,0,1]  0.065 ± 0.0115\n#> 7 [0,1,1]  0.084 ± 0.0116\n#> 8 [1,1,1]  0.187 ± 0.0164\n```\n:::\n\n\n\n## Evaluating DCMs\n\nThere are several functions included for evaluating the model.\nFor example we can examine the probability that each respondent is proficient in each attribute.\nHere, respondent 1 is likely proficient in all skills, whereas respondent 2 is likely only proficient in addition.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlcdm <- add_respondent_estimates(lcdm)\nmeasr_extract(lcdm, \"attribute_prob\")\n#> # A tibble: 1,000 × 4\n#>    resp_id addition multiplication fractions\n#>    <fct>      <dbl>          <dbl>     <dbl>\n#>  1 1       0.997          0.977      0.920  \n#>  2 2       1.00           0.00297    0.0102 \n#>  3 3       1.00           0.133      0.0370 \n#>  4 4       0.0228         1.00       0.0556 \n#>  5 5       0.999          0.0253     0.0223 \n#>  6 6       1.00           0.00297    0.00224\n#>  7 7       0.998          0.469      0.938  \n#>  8 8       0.000520       0.0233     0.0194 \n#>  9 9       0.984          0.000116   0.00641\n#> 10 10      0.997          0.920      0.957  \n#> # ℹ 990 more rows\n```\n:::\n\n\n\nOften, we would dichotomize these probabilities into a 0/1 decision (e.g., proficient/not proficient).\nWe can also look at the reliability of those classifications.\nThe `add_reliability()` function will calculate the classification consistency and accuracy metrics described by [Johnson & Sinharay (2018)](https://doi.org/10.1111/jedm.12196).\nHere we see that all of the attributes have fairly high accuracy and consistency for classifications.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlcdm <- add_reliability(lcdm)\nmeasr_extract(lcdm, \"classification_reliability\")\n#> # A tibble: 3 × 3\n#>   attribute      accuracy consistency\n#>   <chr>             <dbl>       <dbl>\n#> 1 addition          0.960       0.927\n#> 2 multiplication    0.956       0.917\n#> 3 fractions         0.936       0.882\n```\n:::\n\n\n\nAs you can see, for each type of model evaluation, we follow the same process of `add_{metric}` and then we can use `measr_extract()` to view the results.\nFor a complete list of model evaluation options, see `?model_evaluation`.\n\n## Future Development\n\nWe're already in the process of adding features and making improvements for the next version of measr.\nSome highlights include:\n\n* Adding support for more DCM subtypes\n* More refined prior specifications\n* Additional vignettes including a complete description of model evaluation and example case studies\n\nIf you have a specific feature request, suggestion for improvement, or run into any problems, please [open an issue](https://github.com/r-dcm/measr/issues) on the project repository!\n\n## Acknowledgments {.appendix}\n\nThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grant [R305D210045](https://ies.ed.gov/funding/grantsearch/details.asp?ID=4546) to the University of Kansas. The opinions expressed are those of the authors and do not represent the views of the the Institute or the U.S. Department of Education.\n\nFeatured photo by <a href=\"https://unsplash.com/@yer_a_wizard?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Fleur</a> on <a href=\"https://unsplash.com/photos/drafting-instruments-on-top-of-table-dQf7RZhMOJU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>.\n  \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}